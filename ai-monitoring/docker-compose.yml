version: '3.8'

services:
  # Model A - Fast baseline (Llama 3.2 3B)
  ollama-model-a:
    image: ollama/ollama:latest
    container_name: ai-monitoring-ollama-model-a
    ports:
      - "11434:11434"
    volumes:
      - ollama-data-a:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    command: >
      sh -c "ollama serve & sleep 15 && ollama pull llama3.2:3b && echo 'Model A ready' && wait"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 180s
    networks:
      - ai-monitoring-network
    deploy:
      resources:
        limits:
          memory: 6G
        reservations:
          memory: 4G

  # Model B - Premium comparison (Llama 3.3 7B)
  ollama-model-b:
    image: ollama/ollama:latest
    container_name: ai-monitoring-ollama-model-b
    ports:
      - "11435:11434"
    volumes:
      - ollama-data-b:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    command: >
      sh -c "ollama serve & sleep 15 && ollama pull llama3.3:7b && echo 'Model B ready' && wait"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s
    networks:
      - ai-monitoring-network
    deploy:
      resources:
        limits:
          memory: 10G
        reservations:
          memory: 8G

  # Target App - Fragile FastAPI service
  target-app:
    build:
      context: ./target-app
      dockerfile: Dockerfile
    container_name: ai-monitoring-target-app
    ports:
      - "8000:8000"
    volumes:
      - failure-state:/tmp
    environment:
      - DATABASE_URL=${DATABASE_URL:-postgresql://user:pass@fake-db:5432/app}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - FAILURE_STATE_FILE=/tmp/failure_state.json
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - ai-monitoring-network
    restart: unless-stopped

  # Chaos Engine - Failure injection service
  chaos-engine:
    build:
      context: ./chaos-engine
      dockerfile: Dockerfile
    container_name: ai-monitoring-chaos-engine
    volumes:
      - failure-state:/tmp
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      - CHAOS_INTERVAL=${CHAOS_INTERVAL:-180}
      - TARGET_CONTAINER=ai-monitoring-target-app
      - FAILURE_STATE_FILE=/tmp/failure_state.json
      - CHAOS_ENABLED=${CHAOS_ENABLED:-true}
    depends_on:
      target-app:
        condition: service_healthy
    networks:
      - ai-monitoring-network
    restart: unless-stopped

  # MCP Server - Tool interface for Docker and Locust control
  mcp-server:
    build:
      context: ./mcp-server
      dockerfile: Dockerfile
    container_name: ai-monitoring-mcp-server
    ports:
      - "8002:8002"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - failure-state:/tmp
    environment:
      - DOCKER_HOST=unix:///var/run/docker.sock
      - LOCUST_URL=http://locust:8089
      - MCP_PORT=8002
    depends_on:
      - locust
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - ai-monitoring-network
    restart: unless-stopped

  # AI Agent - PydanticAI reasoning engine with A/B model support
  ai-agent:
    build:
      context: ./ai-agent
      dockerfile: Dockerfile
    container_name: ai-monitoring-ai-agent
    ports:
      - "8001:8001"
    environment:
      - MCP_SERVER_URL=http://mcp-server:8002
      - OLLAMA_MODEL_A_URL=http://ollama-model-a:11434
      - OLLAMA_MODEL_B_URL=http://ollama-model-b:11434
      - MODEL_A_NAME=llama3.2:3b
      - MODEL_B_NAME=llama3.3:7b
      - AGENT_PORT=8001
    depends_on:
      ollama-model-a:
        condition: service_healthy
      ollama-model-b:
        condition: service_healthy
      mcp-server:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - ai-monitoring-network
    restart: unless-stopped

  # Locust - Load testing with A/B traffic split
  locust:
    build:
      context: ./locust-tests
      dockerfile: Dockerfile
    container_name: ai-monitoring-locust
    ports:
      - "8089:8089"
    environment:
      - TARGET_APP_URL=http://target-app:8000
      - AI_AGENT_URL=http://ai-agent:8001
      - LOCUST_WEB_PORT=8089
    command: >
      locust
      --web-host 0.0.0.0
      --web-port 8089
      --host http://target-app:8000
    depends_on:
      target-app:
        condition: service_healthy
    networks:
      - ai-monitoring-network
    restart: unless-stopped

  # Streamlit UI - Web interface with repair, chat, and model comparison
  streamlit-ui:
    build:
      context: ./streamlit-ui
      dockerfile: Dockerfile
    container_name: ai-monitoring-streamlit-ui
    ports:
      - "8501:8501"
    environment:
      - AGENT_URL=http://ai-agent:8001
      - MCP_URL=http://mcp-server:8002
      - STREAMLIT_SERVER_PORT=8501
      - STREAMLIT_SERVER_ADDRESS=0.0.0.0
    depends_on:
      ai-agent:
        condition: service_healthy
    networks:
      - ai-monitoring-network
    restart: unless-stopped

networks:
  ai-monitoring-network:
    driver: bridge
    name: ai-monitoring-network

volumes:
  ollama-data-a:
    name: ai-monitoring-ollama-data-a
  ollama-data-b:
    name: ai-monitoring-ollama-data-b
  failure-state:
    name: ai-monitoring-failure-state
