name: aim

services:
  # Model A - Fast baseline (Llama 3.2 3B)
  ollama-model-a:
    build:
      context: .
      dockerfile: Dockerfile.ollama-model-a
    container_name: aim-ollama-model-a
    ports:
      - "11434:11434"
    volumes:
      - ollama-data-a:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      #- NEW_RELIC_LICENSE_KEY=${NEW_RELIC_LICENSE_KEY}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 60s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - aim-network
    deploy:
      resources:
        limits:
          memory: 6G
        reservations:
          memory: 4G

  # Model B - Ultra lightweight (Qwen 2.5 0.5B)
  ollama-model-b:
    build:
      context: .
      dockerfile: Dockerfile.ollama-model-b
    container_name: aim-ollama-model-b
    ports:
      - "11435:11434"
    volumes:
      - ollama-data-b:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      #- NEW_RELIC_LICENSE_KEY=${NEW_RELIC_LICENSE_KEY}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 60s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - aim-network
    deploy:
      resources:
        limits:
          memory: 3G
        reservations:
          memory: 1G

  # Target App - Fragile FastAPI service
  target-app:
    build:
      context: ./target-app
      dockerfile: Dockerfile
    container_name: aim-target-app
    ports:
      - "8000:8000"
    volumes:
      - failure-state:/tmp
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - LOG_LEVEL=${LOG_LEVEL}
      - FAILURE_STATE_FILE=${FAILURE_STATE_FILE}
      #- NEW_RELIC_LICENSE_KEY=${NEW_RELIC_LICENSE_KEY}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - aim-network
    restart: unless-stopped

  # MCP Server - Tool interface for Docker and Locust control
  mcp-server:
    build:
      context: ./mcp-server
      dockerfile: Dockerfile
    container_name: aim-mcp-server
    ports:
      - "8002:8002"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - failure-state:/tmp
    environment:
      - DOCKER_HOST=${DOCKER_HOST}
      - LOCUST_URL=${LOCUST_URL}
      - MCP_PORT=${MCP_PORT}
      - NEW_RELIC_LICENSE_KEY=${NEW_RELIC_LICENSE_KEY}
      - NEW_RELIC_APP_NAME=${NEW_RELIC_APP_NAME_MCP_SERVER}
    depends_on:
      - locust
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 60s
      timeout: 5s
      retries: 3
    networks:
      - aim-network
    restart: unless-stopped

  # AI Agent - PydanticAI reasoning engine with A/B model support
  ai-agent:
    build:
      context: ./ai-agent
      dockerfile: Dockerfile
    container_name: aim-ai-agent
    ports:
      - "8001:8001"
    environment:
      - MCP_SERVER_URL=${MCP_SERVER_URL}
      - OLLAMA_MODEL_A_URL=${OLLAMA_MODEL_A_URL}
      - OLLAMA_MODEL_B_URL=${OLLAMA_MODEL_B_URL}
      - MODEL_A_NAME=${MODEL_A_NAME}
      - MODEL_B_NAME=${MODEL_B_NAME}
      - AGENT_PORT=${AGENT_PORT}
      - NEW_RELIC_LICENSE_KEY=${NEW_RELIC_LICENSE_KEY}
      - NEW_RELIC_APP_NAME=${NEW_RELIC_APP_NAME_AI_AGENT}
    depends_on:
      ollama-model-a:
        condition: service_healthy
      ollama-model-b:
        condition: service_healthy
      mcp-server:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 60s
      timeout: 5s
      retries: 3
    networks:
      - aim-network
    restart: unless-stopped

  # Streamlit UI - Web interface with repair, chat, and model comparison
  streamlit-ui:
    build:
      context: ./streamlit-ui
      dockerfile: Dockerfile
    container_name: aim-streamlit-ui
    ports:
      - "8501:8501"
    environment:
      - AGENT_URL=${AGENT_URL}
      - MCP_URL=${MCP_URL}
      - STREAMLIT_SERVER_PORT=${STREAMLIT_SERVER_PORT}
      - STREAMLIT_SERVER_ADDRESS=${STREAMLIT_SERVER_ADDRESS}
      # New Relic
      - NEW_RELIC_LICENSE_KEY=${NEW_RELIC_LICENSE_KEY}
      - NEW_RELIC_APP_NAME=${NEW_RELIC_APP_NAME_STREAMLIT_UI}
    depends_on:
      ai-agent:
        condition: service_healthy
    networks:
      - aim-network
    restart: unless-stopped

  # Chaos Engine - Failure injection service
  chaos-engine:
    build:
      context: ./chaos-engine
      dockerfile: Dockerfile
    container_name: aim-chaos-engine
    volumes:
      - failure-state:/tmp
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      - CHAOS_INTERVAL=${CHAOS_INTERVAL}
      - TARGET_CONTAINER=${TARGET_CONTAINER}
      - FAILURE_STATE_FILE=${FAILURE_STATE_FILE}
      - CHAOS_ENABLED=${CHAOS_ENABLED}
    depends_on:
      target-app:
        condition: service_healthy
    networks:
      - aim-network
    restart: unless-stopped

  # Locust - Load testing with A/B traffic split
  locust:
    build:
      context: ./locust-tests
      dockerfile: Dockerfile
    container_name: aim-locust
    ports:
      - "8089:8089"
    environment:
      - TARGET_APP_URL=${TARGET_APP_URL}
      - AI_AGENT_URL=${AI_AGENT_URL}
      - LOCUST_WEB_PORT=${LOCUST_WEB_PORT}
    command: >
      --web-host 0.0.0.0
      --web-port 8089
      --host http://target-app:8000
    depends_on:
      target-app:
        condition: service_healthy
    networks:
      - aim-network
    restart: unless-stopped

networks:
  aim-network:
    driver: bridge
    name: aim-network

volumes:
  ollama-data-a:
    name: aim-ollama-data-a
  ollama-data-b:
    name: aim-ollama-data-b
  failure-state:
    name: aim-failure-state
